---
layout:   "post"
title:    "文系の、文系による、文系のためのTensorFlow入門 #1"
category: "Programming"
tags:     ["TensorFlow", "Python"]
---

* 本稿は「[ニューラルネットワークと深層学習](http://nnadl-ja.github.io/nnadl_site_ja/index.html)」を参考にして作成しています。優れた無料のオンライン書籍ですので、ぜひ読んでください。
* 本稿はディープ・ラーニングの素人が書いています。だからあまり信用すんな！　お願い誰かチェックして……。
* 遠回りしまくりで、なかなかTensorFlowには入りません。覚悟しておいてください。

## 人工ニューロン

脳はニューロンで構成されているわけで、だったらそのニューロン相当品を作ってつなげてやれば、脳相当品を作れるんじゃね？　というわけで考え出されたのが、以下に図示すパーセプトロンです。

![]()

パーセプトロンは、コンピューター資源が限られていた1958年に発表されただけあってとても単純です。左側に出ている線が入力で、右側の線が出力。入力も出力も、0か1の2値しかとりません。入力に重みを掛けて足し合わせた結果が閾値を超えたら出力は1に、そうでなければ出力は0になります。

具体例を挙げましょう。朝起きて、会社に行って働くか、それとも行かないで家でダラダラするかを決断するパーセプトロンを作成してみます。入力は、有給休暇が残っているか（残っていれば1）、体調が良いか（良ければ1）、やらなければならない仕事があるか（あれば1）とします。私の魂の声に忠実にこのパーセプトロン相当品をプログラミングすると、以下のコードになります。

```python
def goes_to_office(has_holidays, good_condition, has_work):
  return -3 * has_holidays + 1 * good_condition + 1 * has_work > -1
```

有給があれば（`has_holidays`が1なら）休む、有給がなくなったらしょうがないので会社に行くというロジックです（私の魂の声は、`good_condition`と`has_work`は無視するみたいです）。でも、これだとさすがに社会人失格ですから、自分を曲げて、まだ有給が残っていても、体調が良くて仕事があるなら会社に行かないとね。

```python
def goes_to_office(has_holidays, good_condition, has_work):
  return -3 * has_holidays + 1 * good_condition + 1 * has_work > -2
```

はい、パラメーターをちょっと変えただけで実現できました。人工ニューロン、なかなかやるじゃん。

## フィードフォワード・ニューラル・ネットワーク

とはいえ、人工ニューロン一つだけではできることに限りがありますから、いっぱい繋げてみましょう。単純化のために、いくつか層を作成して、データは一方向にしか流れないものとします。ある層の人工ニューロンの出力が、次の層の人工ニューロンの入力になるわけ。あと、ある層と次の層のニューロンは、全部繋げちゃいます。図にすると、以下のような感じ。

![]()

このニューラル・ネットワークに、0～9の手書きの数字を流すことを考えてみます。たとえばものすごく几帳面な人が書いた手書きの数字の0をスキャナーで読み込むと、以下のようになるでしょう。

```
□□□□□□□□
□□■■■■□□
□■□□□□■□
□■□□□□■□
□■□□□□■□
□■□□□□■□
□□■■■■□□
□□□□□□□□
```

白い四角を0、黒い四角を1とすれば、上の例なら8×8で64個の数値で表現できます。これらの数値が64個の入力として、ニューラル・ネットワークに入ってくるわけです。この入力を処理する人工ニューロンの重みづけと閾値次第では、例えば以下に示す画像の黒い四角の部分が1の場合に1を出力するようにできるでしょう（白い四角の部分は、先ほどの体調の良し悪しや仕事の有無のように、出力には影響しないように重みづけと閾値を調節しておきます）。

```
□□□□□□□□
□□■■□□□□
□■□□□□□□
□■□□□□□□
□□□□□□□□
□□□□□□□□
□□□□□□□□
□□□□□□□□
```

同様に、0の右上部分や、左下部分、右下部分に対して1を出力する人工ニューロンも作れます。

それだけではありません。人工ニューロンがいっぱいあるなら、以下に示すような歪んだ0の一部分に反応する人工ニューロンも作れるでしょう（きれいな0の一部でも歪んだ0の一部でも反応する人工ニューロンがあっても良いです）。

```
□□□□□□□□
□□□■■■□□
□□■□□□■□
□■□□□□■□
□■□□□□■□
□□■□□■□□
□□□■■■□□
□□□□□□□□
```

そして、次の層に、これらの人工ニューロンが1を出力した場合に1を出力する人工ニューロンがあるとしたら？　もしその人工ニューロンが1を出力したなら、入力された手書きの数字は0だと言えるでしょう。8を0と間違えないよう、中央にある横棒に反応する人工ニューロンが1を出力した場合は1を出力しないように、重みづけと閾値が設定されていれば完璧です。

ほら、手書き文字の認識という高度な処理が、とても単純な人工ニューロンのネットワークで実現できました！

## 活性化関数

よっしゃー、数字の0～9の一部分をうまいこと判定する人工ニューロンの重みづけと閾値を手で書いてやるぜぇ……というのは、まぁ、無理ですよね。なんとかして、重みづけと閾値を機械学習させたい。

ここで問題になるのは、重みづけや閾値と出力の関係がぶった切れているといううことです。だって、出力は0か1の2値なんですから。ある日突然離婚を切り出されたので、どこがどう悪かったのか判断できないような感じです。

でも、もし出力が0.836のような実数なら、重みづけや閾値を少し変更すると出力も少し変更するという感じにできるわけです。休日に一人で遊びに出かけたら配偶者が不機嫌になったので、次からは平日に会社を休んで遊びに行こうって感じ。ほら、これなら少しずつ生活態度を調整できるでしょ？

というわけで、人工ニューロンへの入力も出力も実数としましょう。そのために、人工ニューロンの出力を「重みづけした入力の合計が閾値を超えたら1」から、「重みづけした入力の合計にバイアスを足した数」にします。以下のような感じ。

```python
def goes_to_office(has_holidays, good_condition, has_work):
  return -3.0 * has_holidays + 1.0 * good_condition + 1.0 * has_work + 2.0  # 「> -2」を「+ 2.0」に変更しました。
```

でも、まだこれだけでは不十分です。人工ニューロンの出力の変化が一定（線形）だと、ニューラル・ネットワークは単純な処理しかできないらしいんですよ。なので、グラフを描いたときに直線にならない（非線形）結果になるような関数を用意しなければなりません。このような関数を活性化関数と呼びます。

で、私では理屈は分からなかったのですけど、活性化関数はいくつも種類があります。伝統的なのはシグモイド関数、今はやっているのはReLUという奴らしい。どちらもTensorFlowが提供しています。ReLUを使えば間違いはないけど、TensorFlowのAPIを見るといくつも活性化関数が見つかるから、上手くいかないときは適当に入れ替えてみなさいって死んだばっちゃんが言っていました（理屈が分かる人は、理屈に合わせて入れ替えてください）。

## ここまでを、TensorFlowでプログラミングしてみる

なんだ、ニューラル・ネットワークって難しくないじゃん。多数の小人さんが層状に連なって伝言ゲームをしているような感じですな。さっそく作ってみよう……と思うのですけれど、人工ニューロンがいっぱいあって、重みづけもバイアスも多数あって、そんなのを一つづつ変数や関数として書くなんてのは大変すぎます。行列を使って楽をすることにしましょう。

さて、私が行列について知っていることは、行列の掛け算をするときには[積ABが定義されるのはAの列の本数とBの行の本数が一致している場合に限られる](https://ja.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E3%81%AE%E4%B9%97%E6%B3%95)のと、掛け算の結果はAの行とBの列の行列になるってことだけ。

これを先ほどの人工ニューロンの話に適用すると、たとえば1行64列の画像の行列と64行10列の重みづけの行列は掛け算が可能で、その結果は1行10列の行列になるというわけ（しかも、掛け算では数値を掛け算した結果を足し算するという、人工ニューロンのところで書いたプログラムと同じ計算がなされます）。画像を一枚入力に渡せば10個の数値になるわけで、これはある層全体の重みづけ部分の計算そのものです。あと、バイアスを足す部分は、その結果に1行10列の重みづけの行列を足し算すればOK。というわけで、ニューラル・ネットワークの1層分（入力は5個でニューロンの数は3とします）をPythonの数値演算ライブラリのNumPy（まだTensorFlowは使いません）で表現すると、以下のコードになります。

```python
import numpy as np

# データ作成。

x = np.matrix([0.01, 0.02, 0.03, 0.04, 0.05])  # 入力。1行5列。値は適当。
w = np.matrix([[0.06, 0.07, 0.08]              # 重みづけ。5行3列。値は適当。
               [0.09, 0.10, 0.11]
               [0.12, 0.13, 0.14]
               [0.15, 0.16, 0.17]
               [0.18, 0.19, 0.20]])
b = np.matrix([0.21, 0.22, 0.23])              # バイアス。1行3列。値は適当。

# ネットワーク定義。

layer_1_out = x * w + b                        # 1行3列の出力になる。活性化関数は未使用。
```

あら、とっても簡単……。でも、TensorFlowでは、これほど簡単にはいきません。というのも、TensorFlowではどのような計算をするかの定義と、定義した計算の実行を分けて記述しなければならないんですよ。このような面倒な方式になっているのは、学習をさせるにはどのような計算をしたのかという情報が必要になるためです（Chainerというディープ・ラーニングのライブラリでは、コード上での定義と実行が一つになっていますけど、実行時に裏で計算グラフを作っています）。

なので、TensorFlowの流儀に従って、まずはニューラル・ネットワークを定義してみましょう。題材は、28ドット×28ドットの0～9の手書き数字を認識する、MNISTでやります。

```python
# TensorFlowでは、ニューラル・ネットワークの構築はinference()でやるべきだとチュートリアルに書いてあった。
def inference(l0):  # l0は入力となる層。
   
```
